{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import gc\n",
    "import numpy as np\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def processar_arquivos(diretorio_raiz, lista_epocas, tarefas, limite_por_evento=None, quantidade_pessoas=None):\n",
    "    if not os.path.exists(diretorio_raiz):\n",
    "        raise FileNotFoundError(f\"O diretório raiz '{diretorio_raiz}' não foi encontrado.\")\n",
    "\n",
    "    pasta_pessoas = [p for p in os.listdir(diretorio_raiz) if os.path.isdir(os.path.join(diretorio_raiz, p))]\n",
    "    if quantidade_pessoas is not None:\n",
    "        pasta_pessoas = pasta_pessoas[:quantidade_pessoas]\n",
    "    print(\"Quantidade de pessoas:\", len(pasta_pessoas))\n",
    "\n",
    "    canais_desejados = [\n",
    "        'Fp1.', 'F7..', 'F3..', 'T7..', 'C3..', 'P7..', 'P3..', 'O1..',\n",
    "        'Fp2.', 'F4..', 'F8..', 'C4..', 'T8..', 'P4..', 'P8..', 'O2..'\n",
    "    ]\n",
    "\n",
    "    epocas_por_evento = {evento: [] for evento in lista_epocas}\n",
    "    total_arquivos_eventos = {evento: 0 for evento in lista_epocas}\n",
    "\n",
    "    for pessoa in pasta_pessoas:\n",
    "        pasta_pessoa = os.path.join(diretorio_raiz, pessoa)\n",
    "        arquivos_edf = []\n",
    "        for tarefa in tarefas:\n",
    "            arquivos_edf += fnmatch.filter(os.listdir(pasta_pessoa), tarefa)\n",
    "\n",
    "        raws = []\n",
    "        for edf in arquivos_edf:\n",
    "            raw = mne.io.read_raw_edf(os.path.join(pasta_pessoa, edf), preload=True)\n",
    "            raw.pick_channels(canais_desejados)\n",
    "            raws.append(raw)\n",
    "\n",
    "        if raws:\n",
    "            raw_concat = mne.concatenate_raws(raws)\n",
    "            events, event_id = mne.events_from_annotations(raw_concat)\n",
    "            for evento in lista_epocas:\n",
    "                if evento in event_id:\n",
    "                    epochs = mne.Epochs(\n",
    "                        raw_concat, events,\n",
    "                        event_id={evento: event_id[evento]},\n",
    "                        tmin=-0.5, tmax=4, baseline=(-0.5, 0)\n",
    "                    )\n",
    "                    data = (epochs.get_data().astype(np.float32) * 1e6)  # volts→µV\n",
    "                    epocas_por_evento[evento].extend(data)\n",
    "                    total_arquivos_eventos[evento] += len(data)\n",
    "\n",
    "    print(\"\\nQuantidade de arquivos por evento:\")\n",
    "    for ev, cnt in total_arquivos_eventos.items():\n",
    "        print(f\"  {ev}: {cnt}\")\n",
    "\n",
    "    # definir limite_por_evento se não informado\n",
    "    if limite_por_evento is None:\n",
    "        limite_por_evento = int(np.mean([c for c in total_arquivos_eventos.values() if c > 0]))\n",
    "\n",
    "    for ev, arr in epocas_por_evento.items():\n",
    "        if len(arr) > limite_por_evento:\n",
    "            np.random.shuffle(arr)\n",
    "            epocas_por_evento[ev] = arr[:limite_por_evento]\n",
    "            print(f\"{ev} reduzido para {limite_por_evento}\")\n",
    "\n",
    "    return epocas_por_evento\n",
    "\n",
    "def normaliza(tensor):\n",
    "    normalized = []\n",
    "    for sample in tensor:\n",
    "        mn, mx = sample.min(), sample.max()\n",
    "        normalized.append((sample - mn) / (mx - mn + 1e-8))\n",
    "    return np.array(normalized)\n",
    "\n",
    "def separar_dados(epocas_por_evento):\n",
    "    arrays = {ev: np.array(eps) for ev, eps in epocas_por_evento.items()}\n",
    "    data = np.concatenate([v for v in arrays.values() if v.size > 0])\n",
    "    labels = []\n",
    "    for idx, (ev, arr) in enumerate(arrays.items()):\n",
    "        labels += [idx] * len(arr)\n",
    "    x = normaliza(np.nan_to_num(data))\n",
    "    y = np.array(labels)\n",
    "    print(\"x shape:\", x.shape, \" y shape:\", y.shape)\n",
    "    return x, y\n",
    "\n",
    "# Parâmetros e execução\n",
    "diretorio = r\"c:\\Users\\LaBios - BCI\\Documents\\eeg-motor-movementimagery-dataset-1.0.0\\files\"\n",
    "eventos = [\"T1\", \"T2\"]\n",
    "tarefas = [\"*R04.edf\", \"*R08.edf\", \"*R12.edf\"]\n",
    "\n",
    "epocas = processar_arquivos(diretorio, eventos, tarefas, quantidade_pessoas=84)\n",
    "x, y = separar_dados(epocas)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
